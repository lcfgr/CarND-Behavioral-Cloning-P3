{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "# Reads csv\n",
    "def read_data_csv(data_directory, csv_filename, data):\n",
    "    with open(data_directory+csv_filename) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "# Create a new table, by extracting the left and right camera images and compensating steering angle\n",
    "def separate_data_csv(data, correction = 0.20):\n",
    "    separated_data = []\n",
    "    for row in data:\n",
    "        center, left, right, steering, throttle, brake, speed = row\n",
    "        separated_data.append((data_directory+row[0].strip(),float(steering)))\n",
    "        if correction !=0:\n",
    "            separated_data.append((data_directory+row[1].strip(), (float(steering)+correction)))\n",
    "            separated_data.append((data_directory+row[2].strip(), (float(steering)-correction)))           \n",
    "    return separated_data\n",
    "\n",
    "# Create a flipped image (with probability 0.5) and calculate the appropriate angle\n",
    "def random_flip(img, angle):\n",
    "    if random.random()<0.5:\n",
    "        img = cv2.flip(img,1)\n",
    "        angle = -1*angle\n",
    "    return img, angle\n",
    "\n",
    "# Translate image randomly. The ratio of pixel shift was acquired from\n",
    "# https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9\n",
    "def translate_img(img,angle,translate_range = 100):\n",
    "    rows, cols, _ = img.shape\n",
    "    translate_x = translate_range*np.random.uniform()-translate_range/2\n",
    "    angle = angle + translate_x/translate_range*0.4\n",
    "    translate_y = 40*np.random.uniform()-20\n",
    "    Translate_Matrix = np.float32([[1,0,translate_x],[0,1,translate_y]])\n",
    "    img  = cv2.warpAffine(img,Translate_Matrix,(cols,rows))\n",
    "    return img,angle\n",
    "\n",
    "# Any image pre-processing should go here\n",
    "def process_img(img):\n",
    "    return img\n",
    "\n",
    "def generator(data_log, batch_size = 32):\n",
    "    count = 0\n",
    "    images = []\n",
    "    angles = []\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        data_log = shuffle(data_log)\n",
    "        for img_filename, steering in data_log:\n",
    "            # Reject 80% of smaller angles, since we have many straight going images\n",
    "            if steering < 0.5 and random.random()<0.8:\n",
    "                continue\n",
    "            # If have not reached the batch size\n",
    "            if count < batch_size :\n",
    "                img = cv2.imread(img_filename)\n",
    "                \n",
    "                img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "                # Add random translation to the image\n",
    "                img, steering = translate_img(img, steering)\n",
    "                # Flip the image, randomly (0.5 probability)\n",
    "                img,steering = random_flip(img,steering)\n",
    "                # Do any preprocessing needed\n",
    "                img = process_img(img)\n",
    "                images.append(img)\n",
    "                angles.append(steering)\n",
    "                count += 1\n",
    "            else:\n",
    "                images, angles = shuffle(images,angles)\n",
    "                yield np.asarray(images), np.asarray(angles)\n",
    "                #reset all temporary batch parameters to zero\n",
    "                count =0\n",
    "                images = []\n",
    "                angles = []\n",
    "\n",
    "def valid_generator(validation_log,batch_size = 32):\n",
    "    images = []\n",
    "    angles = []\n",
    "    while True:\n",
    "        validation_log = shuffle(validation_log)\n",
    "\n",
    "        for img_filename, steering in validation_log:\n",
    "            img = process_img(cv2.imread(img_filename))\n",
    "            images.append(img)\n",
    "            angles.append(steering)\n",
    "            \n",
    "            if len(images) >= batch_size:\n",
    "                images, angles = shuffle(images, angles)\n",
    "                yield np.asarray(images), np.asarray(angles)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D, Conv2D, Dropout\n",
    "\n",
    "def nvidia_model():\n",
    "    input_shape = (160,320,3)\n",
    "    model = Sequential()\n",
    "    # normalize\n",
    "    model.add(Lambda(lambda x: x/255.0 - 0.5, input_shape = input_shape, output_shape = input_shape))\n",
    "    # crop\n",
    "    model.add(Cropping2D(cropping=((32, 25), (0, 0)),input_shape=(160, 320, 3)))\n",
    "    #50,20 / 60,20\n",
    "    \n",
    "    model.add(Conv2D(3,3,3, subsample=(2,2), border_mode='same',activation='elu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(6,5,5, subsample=(2,2), border_mode='same',activation='elu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(9,5,5, subsample=(2,2), border_mode='same',activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "   # model.add(Conv2D(64,3,3, border_mode='same',activation='elu'))\n",
    "    model.add(Conv2D(12,3,3, border_mode='same',activation='elu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(1164, activation='elu'))\n",
    "    model.add(Dense(100, activation='elu'))\n",
    "    model.add(Dense(50, activation='elu'))\n",
    "    model.add(Dense(10, activation='elu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 160, 320, 3)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)        (None, 103, 320, 3)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 52, 160, 3)    84          cropping2d_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 26, 80, 6)     456         convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 13, 40, 9)     1359        convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 13, 40, 9)     0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 13, 40, 12)    984         dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 13, 40, 12)    0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 6240)          0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           624100      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50)            5050        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 10)            510         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             11          dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 632,554\n",
      "Trainable params: 632,554\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/2\n",
      "12800/12800 [==============================] - 71s - loss: 0.0777 - val_loss: 0.0288\n",
      "Epoch 2/2\n",
      "12800/12800 [==============================] - 71s - loss: 0.0521 - val_loss: 0.0245\n",
      "Saved model 1\n",
      "Epoch 1/2\n",
      "12800/12800 [==============================] - 70s - loss: 0.0455 - val_loss: 0.0205\n",
      "Epoch 2/2\n",
      "12800/12800 [==============================] - 75s - loss: 0.0431 - val_loss: 0.0218\n",
      "Saved model 2\n",
      "Epoch 1/1\n",
      "12800/12800 [==============================] - 76s - loss: 0.0406 - val_loss: 0.0202\n",
      "Saved model 3\n",
      "Epoch 1/1\n",
      "12800/12800 [==============================] - 72s - loss: 0.0378 - val_loss: 0.0207\n",
      "Saved model 4\n",
      "Epoch 1/1\n",
      "12800/12800 [==============================] - 77s - loss: 0.0364 - val_loss: 0.0236\n",
      "Saved model 5\n"
     ]
    }
   ],
   "source": [
    "# Set data folder\n",
    "data_directory = './data/'\n",
    "# read udacity csv\n",
    "data_log = []\n",
    "data_log = read_data_csv(data_directory,'driving_log.csv', data_log)\n",
    "train_log, validation_log = train_test_split(data_log, test_size=0.2)\n",
    "# Process validation log \n",
    "validation_log = separate_data_csv(validation_log, correction = 0)\n",
    "\n",
    "\n",
    "# Process training log, steering correction of left/right images set to 0.25\n",
    "train_log = separate_data_csv(train_log, correction = 0.25)\n",
    "\n",
    "train_generator = generator(train_log, batch_size=64)\n",
    "validation_generator = generator(validation_log, batch_size=64)\n",
    "model = nvidia_model()\n",
    "\n",
    "# Train the model in multiple stages to chose easily a model with smaller overfitting\n",
    "model.fit_generator(train_generator, samples_per_epoch =12800, validation_data = validation_generator, nb_val_samples = 1280, nb_epoch =2)\n",
    "model.save('model1.h5')\n",
    "print(\"Saved model 1\")\n",
    "model.fit_generator(train_generator, samples_per_epoch =12800, validation_data = validation_generator, nb_val_samples = 1280, nb_epoch =2)\n",
    "model.save('model2.h5')\n",
    "print(\"Saved model 2\")\n",
    "model.fit_generator(train_generator, samples_per_epoch =12800, validation_data = validation_generator, nb_val_samples = 1280, nb_epoch =1)\n",
    "model.save('model3.h5')\n",
    "print(\"Saved model 3\")\n",
    "model.fit_generator(train_generator, samples_per_epoch =12800, validation_data = validation_generator, nb_val_samples = 1280, nb_epoch =1)\n",
    "model.save('model4.h5')\n",
    "print(\"Saved model 4\")\n",
    "model.fit_generator(train_generator, samples_per_epoch =12800, validation_data = validation_generator, nb_val_samples = 1280, nb_epoch =1)\n",
    "model.save('model5.h5')\n",
    "print(\"Saved model 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
